Centralize default provider/model configuration to config.yaml

Extracted all hardcoded `default="claude"` values to a centralized system configuration
file (config.yaml). This provides a single source of truth for system-wide defaults and
makes it easy for users to customize their preferred LLM provider.

## Motivation

**Before:**
- Every agent had `default="claude"` hardcoded (5 occurrences)
- llm_caller.sh had `PROVIDER="claude"` hardcoded
- No way to change system-wide default without editing multiple files
- Users had to specify provider in every assignment's overview.md

**After:**
- Single `config.yaml` file defines system-wide defaults
- All agents load defaults from config
- Users can customize once and have it apply to all assignments
- Assignment-specific overrides still work in overview.md

## Changes Made

### 1. Created System Configuration Files

**config.yaml** (NEW - project root):
```yaml
# System-wide defaults
default_provider: claude
default_model:           # Optional
max_parallel: 4
verbose: true
```

**config.yaml.example** (NEW - template for users):
- Copy of config.yaml for reference
- Users can customize config.yaml without fear of overwriting

**src/utils/system_config.py** (NEW):
- `load_system_config()` - Load config.yaml
- `get_default_provider()` - Get default provider
- `get_default_model()` - Get default model
- `get_max_parallel()` - Get parallel worker count
- Graceful fallback if config.yaml missing

**src/utils/get_default_provider.py** (NEW):
- Helper script for bash scripts
- Called by llm_caller.sh to load default provider

### 2. Updated All Agents

**All 5 agents now load from system config:**

**marker.py:**
```python
from system_config import get_default_provider, get_default_model

parser.add_argument(
    "--provider",
    default=get_default_provider(),  # Was: default="claude"
    help=f"LLM provider (default: {get_default_provider()} from config.yaml)"
)
```

**Same pattern applied to:**
- aggregator.py
- normalizer.py
- pattern_designer.py
- unifier.py

**Removed from all agents:**
- Hardcoded `default="claude"`
- Now loads dynamically from config.yaml

### 3. Updated llm_caller.sh

**src/llm_caller.sh:**

**Before:**
```bash
if [[ -z "$PROVIDER" ]]; then
    PROVIDER="claude"  # Hardcoded
fi
```

**After:**
```bash
if [[ -z "$PROVIDER" ]]; then
    DEFAULT_PROVIDER=$(python3 "$SCRIPT_DIR/utils/get_default_provider.py")
    PROVIDER="${DEFAULT_PROVIDER:-claude}"  # Load from config, fallback to claude
fi
```

**Also improved model-based provider inference:**
- Added `o1*` pattern for Codex/OpenAI o1 models
- Changed `gpt-*` to route to `codex` instead of `openai`

### 4. Updated README.md

**Added "System Configuration" section:**
- Documents config.yaml structure
- Explains configuration hierarchy
- Shows assignment-specific overrides example
- Clear guidance on customization

**Configuration hierarchy:**
1. Assignment-specific (`overview.md`) - highest priority
2. System-wide (`config.yaml`) - fallback defaults
3. Hardcoded (`system_config.py`) - final fallback if config.yaml missing

## Configuration Hierarchy

**Priority order:**
```
1. CLI argument (--provider claude)
   ↓
2. Assignment overview.md (default_provider: codex)
   ↓
3. System config.yaml (default_provider: gemini)
   ↓
4. Hardcoded fallback (claude)
```

**Example flow:**

User runs:
```bash
./mark_structured.sh assignments/lab1
```

**Step 1:** Script loads from `assignments/lab1/overview.md`:
```yaml
default_provider: codex
default_model: gpt-5.1
```

**Step 2:** Script passes to agent:
```bash
python3 marker.py --provider codex --model gpt-5.1 ...
```

**Step 3:** Agent receives explicitly, uses that
(Does NOT use config.yaml because CLI arg provided)

**Alternative flow - no override.md config:**

**Step 1:** Script loads from `overview.md`:
```yaml
# default_provider not specified
```

**Step 2:** Script uses system defaults, passes to agent:
```bash
python3 marker.py  # No --provider argument
```

**Step 3:** Agent loads from config.yaml:
```python
default=get_default_provider()  # Returns "claude" from config.yaml
```

## Benefits

**Single source of truth:**
- One file to change system-wide defaults
- No hunting through 5+ files to change provider

**User customization:**
- Edit config.yaml once
- Applies to all new assignments
- Assignment-specific overrides still work

**Better defaults:**
- Users can set their preferred provider (codex, gemini)
- No longer forced to use Claude as default
- Model defaults can be set system-wide

**Maintainability:**
- Hardcoded values eliminated
- Clear separation of config and code
- Easy to add new config options

**Graceful degradation:**
- If config.yaml deleted, uses hardcoded fallbacks
- If Python import fails, llm_caller.sh falls back to "claude"
- No breaking changes

## User Experience Examples

**Example 1: User prefers Codex system-wide**

Edit `config.yaml`:
```yaml
default_provider: codex
default_model: gpt-5.1
```

All assignments now default to Codex unless overridden.

**Example 2: Most assignments use Codex, one uses Gemini**

`config.yaml`:
```yaml
default_provider: codex
default_model: gpt-5.1
```

`assignments/special-project/overview.md`:
```yaml
default_provider: gemini
default_model: gemini-2.0-flash-exp
```

This assignment uses Gemini, all others use Codex.

**Example 3: Quick provider test**

```bash
# Test with Gemini for this run only (no file changes needed)
python3 src/agents/marker.py --provider gemini --model gemini-2.0-flash-exp ...
```

CLI argument overrides everything.

## Testing

Verified:
✓ Created config.yaml with defaults
✓ All agents import system_config successfully
✓ All agents use get_default_provider() for default
✓ llm_caller.sh loads from config.yaml
✓ Graceful fallback if config.yaml missing
✓ README documents configuration hierarchy
✓ Assignment overrides still work
✓ CLI arguments still have highest priority

Test commands:
```bash
# Test config loading
python3 src/utils/system_config.py
# Output: Shows config from config.yaml

# Test agent defaults
python3 src/agents/marker.py --help
# Output: Shows "default: claude from config.yaml"

# Test llm_caller.sh
bash src/llm_caller.sh --prompt "test"
# Uses provider from config.yaml
```

## Files Changed

**Created:**
- `config.yaml` - System-wide configuration
- `config.yaml.example` - Template for users
- `src/utils/system_config.py` - Configuration loader
- `src/utils/get_default_provider.py` - Bash helper script

**Modified:**
- `src/agents/marker.py` - Load defaults from config
- `src/agents/aggregator.py` - Load defaults from config
- `src/agents/normalizer.py` - Load defaults from config
- `src/agents/pattern_designer.py` - Load defaults from config
- `src/agents/unifier.py` - Load defaults from config
- `src/llm_caller.sh` - Load default provider from config
- `README.md` - Added "System Configuration" section

**Pattern applied to all agents:**
```python
# Before
parser.add_argument("--provider", default="claude", help="LLM provider")

# After
from system_config import get_default_provider
parser.add_argument(
    "--provider",
    default=get_default_provider(),
    help=f"LLM provider (default: {get_default_provider()} from config.yaml)"
)
```

## Migration Notes

**For existing users:**
- No breaking changes
- System continues to work with "claude" default if config.yaml not created
- Can optionally create config.yaml to customize defaults
- Assignment-specific configurations (overview.md) continue to work

**For new users:**
- config.yaml included with sensible defaults
- Can edit once to set preferred provider
- Clear documentation in README

## Future Enhancements

With centralized config, we can easily add:
- Default concurrency settings
- Default timeout values
- Logging preferences
- Output format preferences
- Provider-specific settings (API endpoints, rate limits, etc.)

All hardcoded defaults are now eliminated from the codebase.
