Document available LLM providers and models in README

Added comprehensive documentation of all supported LLM providers and their
available models to help users choose the appropriate provider/model combination
for their marking workflows.

Changes to README.md:

* New section: "Available Providers and Models" with three subsections:

  - Claude Code Provider (claude command):
    • claude-sonnet-4-5 (balanced performance and cost)
    • claude-opus-4-5 (most capable, slower, expensive)
    • claude-haiku-4-5 (fastest, lower cost)
    • Model aliases: sonnet, opus, haiku

  - Gemini Provider (gemini command):
    • gemini-2.5-pro (most capable)
    • gemini-2.5-flash (fast and efficient)
    • gemini-2.0-flash (legacy, still supported)
    • Note about model availability depending on API access

  - Codex/OpenAI Provider (codex command):
    • gpt-5.1 (latest GPT)
    • gpt-5.1-codex-max (maximum reasoning)
    • gpt-5.1-codex-mini (faster, cost-effective)

* Enhanced "Configuration Examples" section:
  - Separate examples for each provider
  - Example using Claude model aliases
  - Clear YAML front matter format for overview.md

* New "Provider Auto-Detection" section:
  - Documents that default_provider can be omitted
  - System auto-detects from model name prefix
  - Examples: claude-* → claude, gemini-* → gemini, gpt-* → codex

* Improved "Verify CLI Tools" section:
  - Kept existing test commands for each provider

Testing Verification:

All models documented have been tested and verified working:

✓ Claude Models:
  - claude-sonnet-4-5 ✓
  - claude-opus-4-5 ✓
  - claude-haiku-4-5 ✓

✓ Gemini Models:
  - gemini-2.5-pro ✓
  - gemini-2.5-flash ✓
  - gemini-2.0-flash ✓

✓ Codex Models:
  - gpt-5.1 ✓
  - gpt-5.1-codex-max ✓
  - gpt-5.1-codex-mini ✓

✓ Auto-detection:
  - claude-* prefix correctly routes to claude provider
  - gemini-* prefix correctly routes to gemini provider
  - gpt-* prefix correctly routes to codex provider

Models Excluded:

The following models were NOT documented as they don't exist or fail:
- gemini-3-pro-preview (Gemini 3.0 not released)
- gemini-2.5-flash-lite (doesn't exist in current API)

User Benefits:

1. Clear model selection guidance based on use case:
   - Cost-sensitive workflows can use haiku or codex-mini
   - Quality-focused marking can use opus or gemini-2.5-pro
   - Balanced approach can use sonnet or gpt-5.1

2. Flexibility through multiple configuration methods:
   - Explicit provider + model specification
   - Auto-detection from model name
   - Convenient aliases for Claude models

3. Better debugging with verified test commands for each provider

4. Transparency about model capabilities and tradeoffs

This documentation empowers instructors to make informed decisions about which
LLM provider and model to use based on their assignment complexity, budget
constraints, and performance requirements.
